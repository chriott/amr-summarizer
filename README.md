# AMR Summarization Model

### About

This is an end-to-end AMR-based summarization model. It was created as part of the software project "Applications for AMR" by Lucia Donatelli at Saarland University.

There are two files which each contain all versions of the summarization models: baseline.py and eval.py. 
The implementation in baseline.py is suitable for practical purposes, and the implementation in eval.py is used to evaluate the quality of the summarization models.
As input, baseline.py takes a .txt file containing an English text with each sentence on a separate line. It summarizes this text and outputs the summary into a file with the name 'summary.txt'. 
eval.py takes a file from the proxy report section of the AMR bank corpus, which includes several news reports and their corresponding gold standard summary. It extracts and generates a summary of each news report, and then evaluates the quality of the generated summary compared to the gold standard. We use ROUGE-1, ROUGE-2 and ROUGE-L as our evaluation metrics. An evaluation of each summary is printed to the console, as well as the average scores over the whole corpus.

There are several versions of the model. You will be prompted to choose between them when running the python script. The baseline model works by choosing n sentences from the original text and using these as the summary. In this version, you can have the model use either the first n sentences or choose n random sentences to use for the summary. You will also be prompted to choose the length of the summary in number of sentences.


### Requirements

Before running, you will need to install the following dependencies:

1) amrlib
```
pip3 install amrlib
```
2) penman library
```
pip install penman
```
3) The pretrained spring parser and generator model. Download and un-tar the following models into the amr-summarizer folder, or into the folder containing the baseline.py file.

| Name              	| Version 	| Date       	| Size  	| Score       	| DL 	|
|-------------------	|---------	|------------	|-------	|-------------	|----	|
| parse_spring      	| 0.1.0   	| 2021-11-25 	| 1.5GB 	| 83.5 SMATCH 	| [Link](https://github.com/bjascob/amrlib-models/releases/download/model_parse_spring-v0_1_0/model_parse_spring-v0_1_0.tar.gz)   	|
| generate_t5wtense 	| 0.1.0   	| 2020-12-30 	| 787MB 	| 54/44 BLEU  	| [Link](https://github.com/bjascob/amrlib-models/releases/download/model_generate_t5wtense-v0_1_0/model_generate_t5wtense-v0_1_0.tar.gz)  	|

You will need a stable internet connection to run these models in the code.

3) The AMR coreference model [amr_coref](https://github.com/bjascob/amr_coref)

Currently, there is no pip install of amr_coref, so you will have to clone it:
```
git clone https://github.com/bjascob/amr_coref.git
```
The pre-trained model can be downloaded [here](https://github.com/bjascob/amr_coref/releases). To use the model create a ```data``` directory and un-tar the model in it.

! This dependency is not necessary if you are only using the 'baseline' version of the model.


### Usage

#### Running baseline.py

To run baseline.py, simply run the script with the file containing the text you would like to summarize as an argument:
```
python baseline.py textfile.txt
```
You will be prompted to choose which version of the summarization model to use. Simply enter 'baseline' into the command prompt:
```
baseline
```
You will be prompted to choose between 'random' and 'first'. Choosing 'random' will cause the summarizer to use n random sentences from the input text as the summary. Choosing 'first' will cause the summarizer to use the n first sentences as the summary.
```
random
```
You will then be asked to choose how many sentences the summary should contain. Simply enter any integer:
```
2
```
The summary will then be written to a file called 'summary.txt', which should appear in the folder containing your baseline.py file.


#### Running eval.py


To run eval.py, first install the ROUGE code and word2number
```
pip install rouge_score
pip install word2number
```

Then run the script with AMR bank proxy report file as an argument. We have included two compatible files, _ and _ , which can be used for this task:
```
python eval.py amr-release-3.0-amrs-proxy.txt
```
You will be prompted to choose which version of the summarization model to use. Simply enter 'baseline' into the command prompt:
```
baseline
```
You will be prompted to choose between 'random' and 'first'. Choosing 'random' will cause the summarizer to use n random sentences from the input text as the summary. Choosing 'first' will cause the summarizer to use the n first sentences as the summary.
```
random
```
You will then be asked to choose how many sentences the summary should contain. Simply enter any integer:
```
2
```
For each news article in your proxy report file, the summary generated by our summarizer model will be printed, along with the gold-standard summary. ROUGE-1, ROUGE-2 and ROUGE-L scores will be printed for each generated summary/gold-standard summary pair as well as average scores for the whole corpus.


### Results

Our baseline model achieves these results:

| Random/first              	| n 	| ROUGE-1       	| ROUGE-2  	| ROUGE-L      	|
|-------------------	|---------	|------------	|-------	|-------------	|
| first   	| 1  	| precision: 0.3596 recall: 0.3563 fscore: 0.3378 	| precision: 0.2007 recall: 0.2078 fscore: 0.1925 	| precision: 0.3052 recall: 0.3057 fscore: 0.2880	|
| first 	|  2  	| precision: 0.5134295535783602 recall: 0.2922640833482608 fscore: 0.3536973184879049 	| precision: 0.3021133044091897 recall: 0.16534493999715852 fscore: 0.20348664673386246 	| precision: 0.42859364283633505 recall: 0.24175745903274085 fscore: 0.2937186962998461  	| 
| random | 1 |  precision: 0.19645954485598133 recall: 0.25259787853518767 fscore: 0.20377258594939682 | precision: 0.04888224228904831 recall: 0.0761971344662258 fscore: 0.05554756107463649 | precision: 0.15321711957485049 recall: 0.20229340192983042 fscore: 0.16037338103778664 |
| random | 2 | precision: 0.3059541027451456 recall: 0.18754491998890938 fscore: 0.22032649238043764 | precision: 0.09670580285890461 recall: 0.058506906893058756 fscore: 0.06856845846978386 | precision: 0.22585235283611899 recall: 0.1364586697629532 fscore: 0.16077425702353496 |
